{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine_tuning_on_citeseer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNp1DSxXkJNlAM4eI/Ck2lv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AchrafAsh/gnn-receptive-fields/blob/main/fine_tuning_on_citeseer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqPtRBwhA6IX",
        "outputId": "5c778456-04a4-4aaf-a348-1222a004f99f"
      },
      "source": [
        "import os, sys\n",
        "import os.path as osp\n",
        "from google.colab import drive\n",
        "drive.mount('/content/mnt')\n",
        "nb_path = '/content/notebooks'\n",
        "try:\n",
        "    os.symlink('/content/mnt/My Drive/Colab Notebooks', nb_path)\n",
        "except:\n",
        "    pass\n",
        "sys.path.insert(0, nb_path)  # or append(nb_path)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/mnt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYmrG6x7BH_e"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import copy\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "import yaml\n",
        "\n",
        "from functools import partial\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "from torch_geometric.utils import degree, to_dense_adj, dense_to_sparse, add_self_loops, to_networkx\n",
        "from torch_geometric.nn import GCNConv, MessagePassing, Sequential\n",
        "from torch_sparse import spmm, spspmm\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set_style('whitegrid')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxgwjXUCBJ-9"
      },
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/AchrafAsh/gnn-receptive-fields/main/data.py\n",
        "\n",
        "from data import load_dataset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG8T6njUBKow"
      },
      "source": [
        "## **🎨Designing the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhWT7zZRBOZ5"
      },
      "source": [
        "# Parameter initialization\n",
        "def xavier(tensor):\n",
        "    if tensor is not None:\n",
        "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-2)))\n",
        "        tensor.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "def zeros(tensor):\n",
        "    if tensor is not None:\n",
        "        tensor.data.fill_(0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nApNqGlIBP6D"
      },
      "source": [
        "class OurGCNConv(MessagePassing):\n",
        "    def __init__(self, num_features:int, in_channels:int, out_channels:int, k:int):\n",
        "        super().__init__(aggr='add')  # \"Add\" aggregation\n",
        "        self.k = k\n",
        "        self.lin_neb = torch.nn.Linear(num_features, out_channels)\n",
        "        self.lin_trgt = torch.nn.Linear(in_channels, out_channels)\n",
        "        \n",
        "        self.reset_parameters()\n",
        "        \n",
        "    def reset_parameters(self):\n",
        "        xavier(self.lin_neb.weight)\n",
        "        zeros(self.lin_neb.bias)\n",
        "        \n",
        "        xavier(self.lin_trgt.weight)\n",
        "        zeros(self.lin_trgt.bias)\n",
        "\n",
        "    def forward(self, x, h, edge_index):\n",
        "        # x is the input features and has shape [N, num_features]\n",
        "        # h is the hidden state and has shape [N, in_channels]\n",
        "        # edge_index has shape [2, E] , E being the number of edges\n",
        "\n",
        "        # step 1: linearly transform node feature matrices\n",
        "        x = self.lin_neb(x)\n",
        "        h = self.lin_trgt(h)\n",
        "\n",
        "        # step 3-5: start propagating messages\n",
        "        return self.propagate(next(edge_index), x=x, h=h)\n",
        "\n",
        "    def message(self, x_j, h_i, edge_index, size):\n",
        "        # x_j is the input features of the neighbors and has shape [E, out_channels] (has already been multiplied by the weight matrix)\n",
        "\n",
        "        # step 3: normalize node features\n",
        "        row, col = edge_index\n",
        "        deg = degree(row, size[0], dtype=x_j.dtype)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "\n",
        "        out = norm.view(-1, 1) * x_j\n",
        "\n",
        "        return out + h_i\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        # aggr_out has shape [N, out_channels] is the output of self.message()\n",
        "\n",
        "        # step 5: return new node embeddings\n",
        "        return aggr_out"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUXGw9bYBRxP"
      },
      "source": [
        "# the real deal\n",
        "class OurModel(torch.nn.Module):\n",
        "    def __init__(self, num_layers:int, hidden_dim:int, num_features:int, \n",
        "                 num_classes:int, propagation_steps:int=2, dropout:float=0.5):\n",
        "        super().__init__()\n",
        "        self.propagation_steps = propagation_steps\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.alpha = torch.nn.Parameter(torch.tensor(0.5), requires_grad=True)\n",
        "        # Embedding input features\n",
        "        self.in_mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_features=num_features, out_features=hidden_dim),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "        # Convolutional layers\n",
        "        self.conv_layers = self.create_layers(num_layers=num_layers,\n",
        "                                              hidden_dim=hidden_dim,\n",
        "                                              dropout=dropout)\n",
        "        # Readout function\n",
        "        self.readout = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_features=hidden_dim, out_features=num_classes),\n",
        "            torch.nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def create_layers(self, num_layers:int, hidden_dim:int, dropout:float):\n",
        "        layers = [(OurGCNConv(num_features=hidden_dim, in_channels=hidden_dim, out_channels=hidden_dim, k=0), \"x, x, edge_index -> h\"),\n",
        "                (torch.nn.ReLU(inplace=True)),\n",
        "                (torch.nn.Dropout(p=dropout), \"h -> h\")]\n",
        "        \n",
        "        for k in range(1, num_layers):\n",
        "            layers += [\n",
        "                (OurGCNConv(num_features=hidden_dim, in_channels=hidden_dim, out_channels=hidden_dim, k=k), \"x, h, edge_index -> h\"),\n",
        "                # (GCNConv(hidden_dim, hidden_dim), \"h, edge_index -> h\"),\n",
        "                (torch.nn.ReLU(inplace=True)),\n",
        "                (torch.nn.Dropout(p=dropout), \"h -> h\")\n",
        "            ]\n",
        "        return Sequential(\"x, edge_index\", layers)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv_layers.reset_parameters()\n",
        "\n",
        "\n",
        "    def propagate(self, x, edge_index):\n",
        "        # add self loops\n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "        \n",
        "        # normalize\n",
        "        row, col = edge_index\n",
        "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "\n",
        "        # dense_adj = to_dense_adj(edge_index).squeeze(0)\n",
        "        # edge_index_pow = torch.eye(G.num_nodes).to(device)\n",
        "        # prop_repr = edge_index_pow.clone()\n",
        "        # for _ in range(self.propagation_steps):\n",
        "        #     edge_index_pow = torch.mm(dense_adj, edge_index_pow)\n",
        "        #     prop_repr += edge_index_pow\n",
        "        # return torch.mm(prop_repr, x)\n",
        "        \n",
        "        # APPNP propagation scheme\n",
        "        z = x.clone()\n",
        "        for _ in range(self.propagation_steps):\n",
        "            z = spmm(edge_index, norm, x.size(0), x.size(0), z) * (1-self.alpha) + x * self.alpha\n",
        "        return z\n",
        "\n",
        "        # another propagation scheme (sum of the powers of A)\n",
        "        # props = []\n",
        "        # for _ in range(self.propagation_steps):\n",
        "        #     x= spmm(edge_index, norm, x.size(0), x.size(0), x)\n",
        "        #     props.append(x)\n",
        "        # return sum(props)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # compute different depth edge_index\n",
        "        edge_index_gen = k_hop_neighbors(self.num_layers, G.edge_index)\n",
        "\n",
        "        embeddings = self.in_mlp(x)\n",
        "        h = self.conv_layers(embeddings, edge_index_gen)\n",
        "        out = self.propagate(h, G.edge_index)\n",
        "        return h, self.readout(out)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32WHkuNtBT57"
      },
      "source": [
        "## **🧰Utility functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUkXzluvBbi0"
      },
      "source": [
        "def k_hop_neighbors(k: int, edge_index: torch.Tensor):\n",
        "    \"\"\"Returns the l-hop neighbors for l between 1 (the adjacency matrix) and k (given depth)\n",
        "\n",
        "    Args:\n",
        "        - k (int): size of the maximum neighborhood\n",
        "        - edge_index: sparse matrix of edge indices\n",
        "    \"\"\"\n",
        "    \n",
        "    yield edge_index\n",
        "\n",
        "    \n",
        "    dense_adj = to_dense_adj(edge_index).squeeze(0)\n",
        "    cum_adj = adj_pow = dense_adj\n",
        "\n",
        "    for l in range(1, k):\n",
        "        adj_pow = torch.mm(dense_adj, adj_pow)\n",
        "        k_hop = torch.where(\n",
        "            torch.where(adj_pow > 0, 1, 0) - cum_adj > 0,\n",
        "            1,\n",
        "            0\n",
        "        )\n",
        "        cum_adj += k_hop\n",
        "        \n",
        "        yield dense_to_sparse(k_hop)[0] # return only the edge indices"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSyEHTuHBezZ"
      },
      "source": [
        "# count model parameters\n",
        "def count_parameters(model: torch.nn.Module):\n",
        "    total_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"The model has {total_parameters:,} parameters\")\n",
        "\n",
        "    return total_parameters"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo9ZJplNBfz8"
      },
      "source": [
        "def make(config):\n",
        "    # Make the model\n",
        "    model = OurModel(num_layers=config['num_layers'],\n",
        "                     hidden_dim=config['hidden_dim'],\n",
        "                     num_features=citeseer.num_features,\n",
        "                     num_classes=citeseer.num_classes,\n",
        "                     propagation_steps=config['propagation_steps'],\n",
        "                     dropout=config['dropout']).to(device)\n",
        "\n",
        "    # Make the loss and optimizer\n",
        "    criterion = torch.nn.NLLLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                 lr=config['learning_rate'],\n",
        "                                 weight_decay=config['weight_decay'])\n",
        "    \n",
        "    return model, criterion, optimizer"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrHBmQoQBhax"
      },
      "source": [
        "def train(model, criterion, optimizer, config):\n",
        "    outputs = []\n",
        "    \n",
        "    for _ in range(config['runs']):\n",
        "        for epoch in tqdm(range(config['epochs'])):\n",
        "            loss = train_step(model, optimizer, criterion)\n",
        "            \n",
        "            # test the model\n",
        "            outs = test(model, criterion)\n",
        "                        # metrics=[('mad', mad_value),\n",
        "                        #         ('mad_gap', partial(mad_gap_value,dense_adj)),\n",
        "                        #         ('mad_sp', shortest_path_mad)])\n",
        "            outs['epoch'] = epoch\n",
        "            outs['id'] = config['id']\n",
        "            outs['hidden_dim'] = config['hidden_dim']\n",
        "            outs['weight_decay'] = config['weight_decay']\n",
        "            outs['num_layers'] = config['num_layers']\n",
        "            outs['learning_rate'] = config['learning_rate']\n",
        "            outs['dropout'] = config['dropout']\n",
        "            outs['propagation_steps'] = config['propagation_steps']\n",
        "\n",
        "            outputs.append(outs)\n",
        "\n",
        "    return pd.DataFrame(outputs)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOADLnOaBi9_"
      },
      "source": [
        "def train_step(model, optimizer, criterion):\n",
        "    \"\"\"Performs one training step\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    \n",
        "    # Forward pass\n",
        "    _, out = model(G.x)\n",
        "    loss = criterion(out[G.train_mask], G.y[G.train_mask])\n",
        "    \n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8AqfpLfBkTj"
      },
      "source": [
        "def test(model, criterion, metrics=[]):\n",
        "    \"\"\"\n",
        "    Metrics is a list of tuple ('metric_name', metric_func) where the metric \n",
        "    function takes the last representation matrix and returns a scalar.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Run the model on some test examples\n",
        "    with torch.no_grad():\n",
        "        h, logits = model(G.x)\n",
        "\n",
        "    outs = {}\n",
        "    h = h.detach().cpu()\n",
        "    for (name, metric) in metrics:\n",
        "        outs[name] = metric(h)\n",
        "\n",
        "    for key in ['train', 'val', 'test']:\n",
        "        mask = G[f'{key}_mask']\n",
        "        loss = criterion(logits[mask], G.y[mask]).item()\n",
        "        pred = logits[mask].max(1)[1]\n",
        "        acc = pred.eq(G.y[mask]).sum().item() / mask.sum().item()\n",
        "\n",
        "        outs[f'{key}_loss'] = loss\n",
        "        outs[f'{key}_acc'] = acc\n",
        "    \n",
        "    return outs"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3CtWm_mBl5r"
      },
      "source": [
        "def model_pipeline(config):\n",
        "    # create the model\n",
        "    model, criterion, optimizer = make(config)\n",
        "    config['total_parameters'] = count_parameters(model)\n",
        "\n",
        "    # train the model for different parameters\n",
        "    logs = train(model, criterion, optimizer, config)\n",
        "\n",
        "    # repr = tsne_plot(model, all_edge_index, title=\"Last hidden representations\")\n",
        "\n",
        "    return model, logs, config"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ixOXL-HBnch"
      },
      "source": [
        "## **🕸 Citeseer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbMPjkiPBq63"
      },
      "source": [
        "%%capture\n",
        "path = osp.join(os.getcwd(), 'data')\n",
        "citeseer = load_dataset(path, 'CiteSeer')\n",
        "G = citeseer[0].to(device) # only graph of the dataset\n",
        "palette = sns.color_palette(\"hls\", citeseer.num_classes)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3nUIywRB1nc"
      },
      "source": [
        "## **🔍Fine tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiSOwICEB5Be"
      },
      "source": [
        "grid = dict(\n",
        "    num_layers=[1,2,3],\n",
        "    hidden_dim=[16,24,32],\n",
        "    propagation_steps=[3,6,9],\n",
        "    learning_rate=np.logspace(-4, -1, 5),\n",
        "    weight_decay=np.logspace(-5, 0, 5),\n",
        "    dropout=np.linspace(0.3,0.6, 5),\n",
        "    epochs=200,\n",
        "    runs=2\n",
        ")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "FD7tO02LB65q",
        "outputId": "75f4ebaf-193f-46d0-e4c2-b9f2bbe14f8d"
      },
      "source": [
        "id = 0\n",
        "all_logs=None\n",
        "\n",
        "for num_layers in grid['num_layers']:\n",
        "    for hidden_dim in grid['hidden_dim']:\n",
        "        for propagation_steps in grid['propagation_steps']:\n",
        "            for learning_rate in grid['learning_rate']:\n",
        "                for weight_decay in grid['weight_decay']:\n",
        "                    for dropout in grid['dropout']:\n",
        "                        model, logs, hyperparameters = model_pipeline({\n",
        "                            'id':id,\n",
        "                            'num_layers':num_layers,\n",
        "                            'hidden_dim':hidden_dim,\n",
        "                            'propagation_steps':propagation_steps,\n",
        "                            'learning_rate':learning_rate,\n",
        "                            'weight_decay':weight_decay,\n",
        "                            'dropout':dropout,\n",
        "                            'epochs':200,\n",
        "                            'runs':2\n",
        "                        })\n",
        "\n",
        "                        if id == 0:\n",
        "                            all_logs = logs\n",
        "                        else:\n",
        "                            all_logs = pd.concat([all_logs, logs], ignore_index=True)\n",
        "                        \n",
        "                        id += 1\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 59,911 parameters\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-e04c99b10eed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                             \u001b[0;34m'dropout'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                             \u001b[0;34m'epochs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                             \u001b[0;34m'runs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                         })\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-721b210208d8>\u001b[0m in \u001b[0;36mmodel_pipeline\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# train the model for different parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# repr = tsne_plot(model, all_edge_index, title=\"Last hidden representations\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: train() missing 1 required positional argument: 'config'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8tzW05XB9Yq"
      },
      "source": [
        "all_logs.query('epoch == 199 & train_acc == 1.0 & test_acc > 0.7 & val_acc > 0.7').sort_values(by=[\"test_acc\", \"val_acc\"], ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}