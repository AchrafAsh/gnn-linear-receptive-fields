{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_khop_with_embeddings.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNjyL59uZCipjg+UvSacZGL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AchrafAsh/gnn-receptive-fields/blob/main/06_khop_with_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sclJ_0n4BFKs"
      },
      "source": [
        "After some reflection (and seeing that our model had way too many parameters), we realised something weird happening in the design of Graph Neural Networks.\n",
        "Most GNNs, if not all, are embedding the input features in the first layer which has a receptive field containing only the direct neighbors.\n",
        "It leads to a very biased embedding (100% attention from the direct neighbors) which could potentially cause a loss of information that is irrelevant for the direct neighbors, but highly relevant for remote ones or even for the task at hand.\n",
        "\n",
        "Because our approach required to compute new embeddings for each k-hop neighborhood, we realised that this was not an ideal way of computing embeddings.\n",
        "\n",
        "In this notebook, we try to design a new Message Passing GNN with an initial MLP that serves as an embedding layer from which we then operate our different convolutional layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKZmfT7qLs-2"
      },
      "source": [
        "## **ðŸš€ Setting up the environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orLdyl_jAdh0"
      },
      "source": [
        "import os, sys\n",
        "import os.path as osp\n",
        "from google.colab import drive\n",
        "drive.mount('/content/mnt')\n",
        "nb_path = '/content/notebooks'\n",
        "try:\n",
        "    os.symlink('/content/mnt/My Drive/Colab Notebooks', nb_path)\n",
        "except:\n",
        "    pass\n",
        "sys.path.insert(0, nb_path)  # or append(nb_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaLdo83XCNT6"
      },
      "source": [
        "# import everything\n",
        "import math\n",
        "import random\n",
        "import copy\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "from torch_geometric.utils import to_dense_adj, dense_to_sparse, add_self_loops\n",
        "\n",
        "from torch_geometric.nn import GCNConv, MessagePassing, Sequential\n",
        "from torch_geometric.utils import degree, to_dense_adj\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set_style('darkgrid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9je4QMSYLBXt"
      },
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBMGdT6_Cdpg"
      },
      "source": [
        "def get_k_neighbors(k: int):\n",
        "    \"\"\"Returns the l-hop neighbors for l between 1 (the adjacency matrix) and k (given depth)\n",
        "\n",
        "    Args:\n",
        "        - adj: dense adjacency matrix\n",
        "        - k (int): size of the maximum neighborhood\n",
        "    \"\"\"\n",
        "    \n",
        "    output = [G.edge_index]\n",
        "    dense_adj = to_dense_adj(G.edge_index).squeeze(0)\n",
        "    dense_nebs = [dense_adj.clone()]\n",
        "    adj_pow = dense_adj.clone()\n",
        "\n",
        "    for l in tqdm(range(1, k)):\n",
        "        adj_pow = torch.mm(dense_adj, adj_pow)\n",
        "        k_neb = torch.where(\n",
        "            torch.where(adj_pow > 0, 1, 0) - sum(dense_nebs) > 0,\n",
        "            1,\n",
        "            0\n",
        "        )\n",
        "        dense_nebs.append(k_neb)\n",
        "        output.append(dense_to_sparse(k_neb)[0])\n",
        "    \n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PRqeb7NLFPB"
      },
      "source": [
        "def model_pipeline(hyperparameters):\n",
        "    with wandb.init(project='gnn-receptive-fields', entity='achraf', config=hyperparameters):\n",
        "        config = wandb.config # to make sure we use what is saved in wandb\n",
        "\n",
        "        # create the model\n",
        "        model, criterion, optimizer = make(config)\n",
        "        \n",
        "        # compute different depth edge_index\n",
        "        all_edge_index = get_k_neighbors(config.num_layers)\n",
        "\n",
        "        # train the model\n",
        "        train(model, all_edge_index, criterion, optimizer, config)\n",
        "\n",
        "        # Save the model in the exchangeable ONNX format â†’ not working... yet\n",
        "        # torch.onnx.export(model, G, \"model.onnx\")\n",
        "        # wandb.save(\"model.onnx\")\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoOwBE0ICew_"
      },
      "source": [
        "# Parameter initialization\n",
        "\n",
        "def xavier(tensor):\n",
        "    \"\"\"Initialize weight matrix with Xavier distribution\n",
        "\n",
        "    Args:\n",
        "        tensor (tensor): weigh matrix\n",
        "    Return:\n",
        "        tensor - weight matrix initialized\n",
        "    \"\"\"\n",
        "    if tensor is not None:\n",
        "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-2)))\n",
        "        tensor.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "def zeros(tensor):\n",
        "    \"\"\"Initialize bias vector with all zeros\n",
        "\n",
        "    Args:\n",
        "        tensor (tensor): bias vector\n",
        "    \n",
        "    Return\n",
        "        tensor - bias vector initialized with zeros\n",
        "    \"\"\"\n",
        "    if tensor is not None:\n",
        "        tensor.data.fill_(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZFgW_W4Looj"
      },
      "source": [
        "## **ðŸŽ¨ Designing the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jQEIzoIIsk8"
      },
      "source": [
        "class HopGCNConv(MessagePassing):\n",
        "    def __init__(self, num_features:int, in_channels:int, out_channels:int, k:int):\n",
        "        super(HopGCNConv, self).__init__(aggr='add')  # \"Add\" aggregation\n",
        "        self.k = k\n",
        "        self.lin_input = torch.nn.Linear(num_features, out_channels)\n",
        "        self.lin_hidden = torch.nn.Linear(in_channels, out_channels)\n",
        "        \n",
        "        self.reset_parameters()\n",
        "        \n",
        "    def reset_parameters(self):\n",
        "        xavier(self.lin_input.weight)\n",
        "        zeros(self.lin_input.bias)\n",
        "        \n",
        "        xavier(self.lin_hidden.weight)\n",
        "        zeros(self.lin_hidden.bias)\n",
        "\n",
        "    def forward(self, x, h, edge_index):\n",
        "        # x is the input features and has shape [N, num_features]\n",
        "        # h is the hidden state and has shape [N, in_channels]\n",
        "        # edge_index has shape [2, E] , E being the number of edges\n",
        "\n",
        "        # step 1: linearly transform node feature matrices\n",
        "        x = self.lin_input(x)\n",
        "        h = self.lin_hidden(h)\n",
        "\n",
        "        # step 3-5: start propagating messages\n",
        "        return self.propagate(edge_index[self.k], x=x, h=h)\n",
        "\n",
        "    def message(self, x_j, h_i, edge_index, size):\n",
        "        # x_j is the input features of the neighbors and has shape [E, out_channels] (has already been multiplied by the weight matrix)\n",
        "\n",
        "        # step 3: normalize node features\n",
        "        row, col = edge_index\n",
        "        deg = degree(row, size[0], dtype=x_j.dtype)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "\n",
        "        out = norm.view(-1, 1) * x_j\n",
        "\n",
        "        return out + h_i\n",
        "        # add the hidden state of the target node\n",
        "        # norm = deg_inv_sqrt[row] * deg_inv_sqrt[row]\n",
        "        \n",
        "        # return out + norm.view(-1, 1) * h_i\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        # aggr_out has shape [N, out_channels] is the output of self.message()\n",
        "\n",
        "        # step 5: return new node embeddings\n",
        "        return aggr_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ6WNylDIuw9"
      },
      "source": [
        "# the real deal\n",
        "class KHopNet(torch.nn.Module):\n",
        "    def __init__(self, num_layers:int, hidden_dim:int, num_features:int, num_classes:int, dropout:float=0.5, invert:bool=False):\n",
        "        super().__init__()\n",
        "        self.in_mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_features=num_features, out_features=hidden_dim),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.conv_layers = self.create_layers(num_layers=num_layers,\n",
        "                                              num_classes=num_classes,\n",
        "                                              hidden_dim=hidden_dim,\n",
        "                                              dropout=dropout,\n",
        "                                              invert=invert)\n",
        "        \n",
        "        self.out_mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_features=hidden_dim, out_features=num_classes),\n",
        "            torch.nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def create_layers(self, num_layers:int, num_features:int, num_classes:int, \n",
        "                      hidden_dim:int, dropout:float, invert:bool):\n",
        "        layers = []\n",
        "        for k in range(num_layers):\n",
        "            layers += [\n",
        "                (HopGCNConv(num_features=num_features, in_channels=hidden_dim, out_channels=hidden_dim, k=k), \"x, h, edge_index -> h\"),\n",
        "                (torch.nn.ReLU(), \"h -> h\"),\n",
        "                (torch.nn.Dropout(p=dropout), \"h -> h\")\n",
        "            ]\n",
        "        return Sequential(\"x, edge_index\", layers)\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv_layers.reset_parameters()\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        embeddings = self.in_mlp(x)\n",
        "        h = self.conv_layers(embeddings, edge_index)\n",
        "        return h, self.out_mlp(h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPDFbtutLKIA"
      },
      "source": [
        "def make(config):\n",
        "    # Make the model\n",
        "    model = KHopNet(num_layers=config.num_layers, hidden_dim=config.hidden_dim,\n",
        "                    num_features=cora_dataset.num_features,\n",
        "                    num_classes=cora_dataset.num_classes,\n",
        "                    dropout=config.dropout).to(device)\n",
        "\n",
        "    # Make the loss and optimizer\n",
        "    criterion = torch.nn.NLLLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                 lr=config.learning_rate,\n",
        "                                 weight_decay=config.weight_decay)\n",
        "    \n",
        "    return model, criterion, optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNftl2EJKtaS"
      },
      "source": [
        "## **ðŸ§° Utility functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbRr8MlyKunT"
      },
      "source": [
        "# count model parameters\n",
        "def count_parameters(model: torch.nn.Module):\n",
        "    print(f\"The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} parameters\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BjffiwYK3d9"
      },
      "source": [
        "def train(model, all_edge_index, criterion, optimizer, config):\n",
        "    # tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
        "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "\n",
        "    # Run training and track with wandb\n",
        "    for epoch in tqdm(range(config.epochs)):\n",
        "        loss = train_step(model, all_edge_index, optimizer, criterion)\n",
        "        \n",
        "        # test the model\n",
        "        test(model, all_edge_index)\n",
        "\n",
        "\n",
        "def train_step(model, all_edge_index, optimizer, criterion):\n",
        "    \"\"\"Performs one training step\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    \n",
        "    # Forward pass\n",
        "    _, out = model(G.x, all_edge_index)\n",
        "    loss = criterion(out[G.train_mask], G.y[G.train_mask])\n",
        "    \n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NoRsdlxLQ31"
      },
      "source": [
        "def test(model, all_edge_index, metrics=[]):\n",
        "    \"\"\"\n",
        "    Metrics is a list of tuple ('metric_name', metric_func) where the metric \n",
        "    function takes the last representation matrix and returns a scalar.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Run the model on some test examples\n",
        "    with torch.no_grad():\n",
        "        h, logits = model(G.x, all_edge_index)\n",
        "\n",
        "    outs = {}\n",
        "    for (name, metric) in metrics:\n",
        "        outs[name] = metric(h)\n",
        "\n",
        "    for key in ['train', 'val', 'test']:\n",
        "        mask = G[f'{key}_mask']\n",
        "        loss = F.nll_loss(logits[mask], G.y[mask]).item()\n",
        "        pred = logits[mask].max(1)[1]\n",
        "        acc = pred.eq(G.y[mask]).sum().item() / mask.sum().item()\n",
        "\n",
        "        outs[f'{key}_loss'] = loss\n",
        "        outs[f'{key}_acc'] = acc\n",
        "    \n",
        "    wandb.log(outs)\n",
        "    \n",
        "    return outs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJnS7eE8LS7q"
      },
      "source": [
        "config = dict(\n",
        "    num_layers=8,\n",
        "    hidden_dim=16,\n",
        "    epochs=1000,\n",
        "    learning_rate=0.001,\n",
        "    weight_decay=1e-5,\n",
        "    dropout=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbw0zc9ZLUge"
      },
      "source": [
        "## **ðŸ§ª Run experiments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jKb3rh4LYNU"
      },
      "source": [
        "model = model_pipeline(config)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}