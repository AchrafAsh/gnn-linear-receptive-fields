{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine_tuning_on_cora.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOkwbseIlzCKTHiiiMQicwa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AchrafAsh/gnn-receptive-fields/blob/main/fine_tuning_on_cora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr7qeoQ_wWK_"
      },
      "source": [
        "In this notebook, we attempt to fine-tune our architecture (modified GCN with propagation steps) on Cora dataset.\n",
        "We'll use the suboptimal grid search method as we lack time to look for better methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TxfxJYcwTOB"
      },
      "source": [
        "## **🚀Setting up**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwkFuA4KwEvu",
        "outputId": "d7c39cef-0bc8-490d-e83c-da19970fc109"
      },
      "source": [
        "import os, sys\n",
        "import os.path as osp\n",
        "from google.colab import drive\n",
        "drive.mount('/content/mnt')\n",
        "nb_path = '/content/notebooks'\n",
        "try:\n",
        "    os.symlink('/content/mnt/My Drive/Colab Notebooks', nb_path)\n",
        "except:\n",
        "    pass\n",
        "sys.path.insert(0, nb_path)  # or append(nb_path)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/mnt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMm556sbwO2V"
      },
      "source": [
        "# import everything\n",
        "import math\n",
        "import random\n",
        "import copy\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "import yaml\n",
        "\n",
        "from functools import partial\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "from torch_geometric.utils import degree, to_dense_adj, dense_to_sparse, add_self_loops, to_networkx\n",
        "from torch_geometric.nn import GCNConv, MessagePassing, Sequential\n",
        "from torch_sparse import spmm, spspmm\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set_style('whitegrid')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCwZgrJdwQYb"
      },
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/AchrafAsh/gnn-receptive-fields/main/data.py\n",
        "\n",
        "from data import load_dataset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogVkLFaWwjwl"
      },
      "source": [
        "## **🎨Designing the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW30Yy5swp61"
      },
      "source": [
        "# Parameter initialization\n",
        "def xavier(tensor):\n",
        "    if tensor is not None:\n",
        "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-2)))\n",
        "        tensor.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "def zeros(tensor):\n",
        "    if tensor is not None:\n",
        "        tensor.data.fill_(0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuuLRhtuwrSr"
      },
      "source": [
        "class OurGCNConv(MessagePassing):\n",
        "    def __init__(self, num_features:int, in_channels:int, out_channels:int, k:int):\n",
        "        super().__init__(aggr='add')  # \"Add\" aggregation\n",
        "        self.k = k\n",
        "        self.lin_neb = torch.nn.Linear(num_features, out_channels)\n",
        "        self.lin_trgt = torch.nn.Linear(in_channels, out_channels)\n",
        "        \n",
        "        self.reset_parameters()\n",
        "        \n",
        "    def reset_parameters(self):\n",
        "        xavier(self.lin_neb.weight)\n",
        "        zeros(self.lin_neb.bias)\n",
        "        \n",
        "        xavier(self.lin_trgt.weight)\n",
        "        zeros(self.lin_trgt.bias)\n",
        "\n",
        "    def forward(self, x, h, edge_index):\n",
        "        # x is the input features and has shape [N, num_features]\n",
        "        # h is the hidden state and has shape [N, in_channels]\n",
        "        # edge_index has shape [2, E] , E being the number of edges\n",
        "\n",
        "        # step 1: linearly transform node feature matrices\n",
        "        x = self.lin_neb(x)\n",
        "        h = self.lin_trgt(h)\n",
        "\n",
        "        # step 3-5: start propagating messages\n",
        "        return self.propagate(edge_index[self.k].to(device), x=x, h=h)\n",
        "\n",
        "    def message(self, x_j, h_i, edge_index, size):\n",
        "        # x_j is the input features of the neighbors and has shape [E, out_channels] (has already been multiplied by the weight matrix)\n",
        "\n",
        "        # step 3: normalize node features\n",
        "        row, col = edge_index\n",
        "        deg = degree(row, size[0], dtype=x_j.dtype)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "\n",
        "        out = norm.view(-1, 1) * x_j\n",
        "\n",
        "        return out + h_i\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        # aggr_out has shape [N, out_channels] is the output of self.message()\n",
        "\n",
        "        # step 5: return new node embeddings\n",
        "        return aggr_out"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ7yVxevwtIp"
      },
      "source": [
        "# the real deal\n",
        "class OurModel(torch.nn.Module):\n",
        "    def __init__(self, num_layers:int, hidden_dim:int, num_features:int, \n",
        "                 num_classes:int, propagation_steps:int=2, dropout:float=0.5):\n",
        "        super().__init__()\n",
        "        self.propagation_steps = propagation_steps\n",
        "        \n",
        "        self.alpha = torch.nn.Parameter(torch.tensor(0.5), requires_grad=True)\n",
        "        # Embedding input features\n",
        "        self.in_mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_features=num_features, out_features=hidden_dim),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "        # Convolutional layers\n",
        "        self.conv_layers = self.create_layers(num_layers=num_layers,\n",
        "                                              hidden_dim=hidden_dim,\n",
        "                                              dropout=dropout)\n",
        "        # Readout function\n",
        "        self.readout = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_features=hidden_dim, out_features=num_classes),\n",
        "            torch.nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def create_layers(self, num_layers:int, hidden_dim:int, dropout:float):\n",
        "        layers = [(OurGCNConv(num_features=hidden_dim, in_channels=hidden_dim, out_channels=hidden_dim, k=0), \"x, x, edge_index -> h\"),\n",
        "                (torch.nn.ReLU(inplace=True)),\n",
        "                (torch.nn.Dropout(p=dropout), \"h -> h\")]\n",
        "        \n",
        "        for k in range(1, num_layers):\n",
        "            layers += [\n",
        "                (OurGCNConv(num_features=hidden_dim, in_channels=hidden_dim, out_channels=hidden_dim, k=k), \"x, h, edge_index -> h\"),\n",
        "                # (GCNConv(hidden_dim, hidden_dim), \"h, edge_index -> h\"),\n",
        "                (torch.nn.ReLU(inplace=True)),\n",
        "                (torch.nn.Dropout(p=dropout), \"h -> h\")\n",
        "            ]\n",
        "        return Sequential(\"x, edge_index\", layers)\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv_layers.reset_parameters()\n",
        "\n",
        "\n",
        "    def propagate(self, x, edge_index):\n",
        "        # add self loops\n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "        \n",
        "        # normalize\n",
        "        row, col = edge_index\n",
        "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "\n",
        "        # dense_adj = to_dense_adj(edge_index).squeeze(0)\n",
        "        # edge_index_pow = torch.eye(G.num_nodes).to(device)\n",
        "        # prop_repr = edge_index_pow.clone()\n",
        "        # for _ in range(self.propagation_steps):\n",
        "        #     edge_index_pow = torch.mm(dense_adj, edge_index_pow)\n",
        "        #     prop_repr += edge_index_pow\n",
        "        # return torch.mm(prop_repr, x)\n",
        "        \n",
        "        # APPNP propagation scheme\n",
        "        z = x.clone()\n",
        "        for _ in range(self.propagation_steps):\n",
        "            z = spmm(edge_index, norm, x.size(0), x.size(0), z) * (1-self.alpha) + x * self.alpha\n",
        "        return z\n",
        "\n",
        "        # another propagation scheme (sum of the powers of A)\n",
        "        # props = []\n",
        "        # for _ in range(self.propagation_steps):\n",
        "        #     x= spmm(edge_index, norm, x.size(0), x.size(0), x)\n",
        "        #     props.append(x)\n",
        "        # return sum(props)\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        embeddings = self.in_mlp(x)\n",
        "        h = self.conv_layers(embeddings, edge_index)\n",
        "        out = self.propagate(h, edge_index[0].to(device))\n",
        "        return h, self.readout(out)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xszn3hhwv8I"
      },
      "source": [
        "## **🧰Utility functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKCOx2w8w074"
      },
      "source": [
        "def get_k_neighbors(k: int):\n",
        "    \"\"\"Returns the l-hop neighbors for l between 1 (the adjacency matrix) and k (given depth)\n",
        "\n",
        "    Args:\n",
        "        - k (int): size of the maximum neighborhood\n",
        "    \"\"\"\n",
        "    \n",
        "    output = [G.edge_index]\n",
        "    dense_adj = to_dense_adj(G.edge_index).squeeze(0)\n",
        "    dense_nebs = [dense_adj.clone()]\n",
        "    adj_pow = dense_adj.clone()\n",
        "\n",
        "    for l in tqdm(range(1, k)):\n",
        "        adj_pow = torch.mm(dense_adj, adj_pow)\n",
        "        k_neb = torch.where(\n",
        "            torch.where(adj_pow > 0, 1, 0) - sum(dense_nebs) > 0,\n",
        "            1,\n",
        "            0\n",
        "        )\n",
        "        dense_nebs.append(k_neb)\n",
        "        output.append(dense_to_sparse(k_neb)[0])\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F420NyXHw35S"
      },
      "source": [
        "# count model parameters\n",
        "def count_parameters(model: torch.nn.Module):\n",
        "    total_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"The model has {total_parameters:,} parameters\")\n",
        "\n",
        "    return total_parameters"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9alC_myw5zm"
      },
      "source": [
        "def make(config):\n",
        "    # Make the model\n",
        "    model = OurModel(num_layers=config['num_layers'],\n",
        "                     hidden_dim=config['hidden_dim'],\n",
        "                     num_features=cora_dataset.num_features,\n",
        "                     num_classes=cora_dataset.num_classes,\n",
        "                     propagation_steps=config['propagation_steps'],\n",
        "                     dropout=config['dropout']).to(device)\n",
        "\n",
        "    # Make the loss and optimizer\n",
        "    criterion = torch.nn.NLLLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                 lr=config['learning_rate'],\n",
        "                                 weight_decay=config['weight_decay'])\n",
        "    \n",
        "    return model, criterion, optimizer"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "662RlCxaw7c_"
      },
      "source": [
        "def train(model, all_edge_index, criterion, optimizer, config):\n",
        "    dense_adj = to_dense_adj(G.edge_index).squeeze(0)\n",
        "\n",
        "    outputs = []\n",
        "    \n",
        "    for _ in range(config['runs']):\n",
        "        for epoch in tqdm(range(config['epochs'])):\n",
        "            loss = train_step(model, all_edge_index, optimizer, criterion)\n",
        "            \n",
        "            # test the model\n",
        "            outs = test(model, all_edge_index, criterion)\n",
        "                        # metrics=[('mad', mad_value),\n",
        "                        #         ('mad_gap', partial(mad_gap_value,dense_adj)),\n",
        "                        #         ('mad_sp', shortest_path_mad)])\n",
        "            outs['epoch'] = epoch\n",
        "            outs['id'] = config['id']\n",
        "            outs['hidden_dim'] = config['hidden_dim']\n",
        "            outs['weight_decay'] = config['weight_decay']\n",
        "            outs['num_layers'] = config['num_layers']\n",
        "            outs['learning_rate'] = config['learning_rate']\n",
        "            outs['dropout'] = config['dropout']\n",
        "            outs['propagation_steps'] = config['propagation_steps']\n",
        "\n",
        "            outputs.append(outs)\n",
        "\n",
        "    return pd.DataFrame(outputs)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8MRXY7Rw9NM"
      },
      "source": [
        "def train_step(model, all_edge_index, optimizer, criterion):\n",
        "    \"\"\"Performs one training step\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    \n",
        "    # Forward pass\n",
        "    _, out = model(G.x.to(device), all_edge_index)\n",
        "    loss = criterion(out[G.train_mask], G.y[G.train_mask])\n",
        "    \n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c2Zd9jHw-_F"
      },
      "source": [
        "def test(model, all_edge_index, criterion, metrics=[]):\n",
        "    \"\"\"\n",
        "    Metrics is a list of tuple ('metric_name', metric_func) where the metric \n",
        "    function takes the last representation matrix and returns a scalar.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Run the model on some test examples\n",
        "    with torch.no_grad():\n",
        "        h, logits = model(G.x, all_edge_index)\n",
        "\n",
        "    outs = {}\n",
        "    h = h.detach().cpu()\n",
        "    for (name, metric) in metrics:\n",
        "        outs[name] = metric(h)\n",
        "\n",
        "    for key in ['train', 'val', 'test']:\n",
        "        mask = G[f'{key}_mask']\n",
        "        loss = criterion(logits[mask], G.y[mask]).item()\n",
        "        pred = logits[mask].max(1)[1]\n",
        "        acc = pred.eq(G.y[mask]).sum().item() / mask.sum().item()\n",
        "\n",
        "        outs[f'{key}_loss'] = loss\n",
        "        outs[f'{key}_acc'] = acc\n",
        "    \n",
        "    return outs"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhqI6ux3xAsq"
      },
      "source": [
        "def model_pipeline(config):\n",
        "    # create the model\n",
        "    model, criterion, optimizer = make(config)\n",
        "    config['total_parameters'] = count_parameters(model)\n",
        "    \n",
        "    # compute different depth edge_index\n",
        "    all_edge_index = get_k_neighbors(config['num_layers'])\n",
        "\n",
        "    # train the model for different parameters\n",
        "    logs = train(model, all_edge_index, criterion, optimizer, config)\n",
        "\n",
        "    # repr = tsne_plot(model, all_edge_index, title=\"Last hidden representations\")\n",
        "\n",
        "    return model, logs, config"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJOuyp2fxDUZ"
      },
      "source": [
        "## **🕸 Cora**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIS0sGoOxGqh"
      },
      "source": [
        "%%capture\n",
        "path = osp.join(os.getcwd(), 'data')\n",
        "cora_dataset = load_dataset(path, 'Cora')\n",
        "G = cora_dataset[0].to(device) # only graph of the dataset\n",
        "palette = sns.color_palette(\"hls\", cora_dataset.num_classes)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tciCvLY2xVh9"
      },
      "source": [
        "## **🔍Fine tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inXNjXDLpurv"
      },
      "source": [
        "grid = dict(\n",
        "    num_layers=[1],\n",
        "    hidden_dim=[16],\n",
        "    propagation_steps=[7,8,9],\n",
        "    learning_rate=np.linspace(1e-3, 8e-3, 5),\n",
        "    weight_decay=np.logspace(-3, -2, 5),\n",
        "    dropout=np.linspace(0.4,0.5, 6),\n",
        "    epochs=200,\n",
        "    runs=5\n",
        ")"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFzylpQxpurx"
      },
      "source": [
        "id = 0\n",
        "all_logs=None\n",
        "\n",
        "for num_layers in grid['num_layers']:\n",
        "    for hidden_dim in grid['hidden_dim']:\n",
        "        for propagation_steps in grid['propagation_steps']:\n",
        "            for learning_rate in grid['learning_rate']:\n",
        "                for weight_decay in grid['weight_decay']:\n",
        "                    for dropout in grid['dropout']:\n",
        "                        model, logs, hyperparameters = model_pipeline({\n",
        "                            'id':id,\n",
        "                            'num_layers':num_layers,\n",
        "                            'hidden_dim':hidden_dim,\n",
        "                            'propagation_steps':propagation_steps,\n",
        "                            'learning_rate':learning_rate,\n",
        "                            'weight_decay':weight_decay,\n",
        "                            'dropout':dropout,\n",
        "                            'epochs':200,\n",
        "                            'runs':2\n",
        "                        })\n",
        "\n",
        "                        if id == 0:\n",
        "                            all_logs = logs\n",
        "                        else:\n",
        "                            all_logs = pd.concat([all_logs, logs], ignore_index=True)\n",
        "                        \n",
        "                        id += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl9RvCQQt-YB",
        "outputId": "2c401e37-420e-4b8f-ebda-418d84bc84af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "all_logs.query('epoch == 199 & train_acc == 1.0 & test_acc > 0.81 & val_acc > 0.76').sort_values(by=[\"test_acc\", \"val_acc\"], ascending=False)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_loss</th>\n",
              "      <th>train_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>test_loss</th>\n",
              "      <th>test_acc</th>\n",
              "      <th>epoch</th>\n",
              "      <th>id</th>\n",
              "      <th>hidden_dim</th>\n",
              "      <th>weight_decay</th>\n",
              "      <th>num_layers</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>dropout</th>\n",
              "      <th>propagation_steps</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>59599</th>\n",
              "      <td>0.018396</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.776217</td>\n",
              "      <td>0.792</td>\n",
              "      <td>0.640162</td>\n",
              "      <td>0.818</td>\n",
              "      <td>199</td>\n",
              "      <td>148</td>\n",
              "      <td>16</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0080</td>\n",
              "      <td>0.48</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155399</th>\n",
              "      <td>0.023995</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.773973</td>\n",
              "      <td>0.786</td>\n",
              "      <td>0.665278</td>\n",
              "      <td>0.816</td>\n",
              "      <td>199</td>\n",
              "      <td>388</td>\n",
              "      <td>16</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.48</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113399</th>\n",
              "      <td>0.005760</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.918060</td>\n",
              "      <td>0.794</td>\n",
              "      <td>0.774391</td>\n",
              "      <td>0.811</td>\n",
              "      <td>199</td>\n",
              "      <td>283</td>\n",
              "      <td>16</td>\n",
              "      <td>0.003162</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0080</td>\n",
              "      <td>0.42</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179999</th>\n",
              "      <td>0.022396</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.786990</td>\n",
              "      <td>0.770</td>\n",
              "      <td>0.654397</td>\n",
              "      <td>0.811</td>\n",
              "      <td>199</td>\n",
              "      <td>449</td>\n",
              "      <td>16</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0080</td>\n",
              "      <td>0.50</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        train_loss  train_acc  ...  dropout  propagation_steps\n",
              "59599     0.018396        1.0  ...     0.48                  7\n",
              "155399    0.023995        1.0  ...     0.48                  9\n",
              "113399    0.005760        1.0  ...     0.42                  8\n",
              "179999    0.022396        1.0  ...     0.50                  9\n",
              "\n",
              "[4 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    }
  ]
}